{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Airline Tweet Sentiment Analysis**"
      ],
      "metadata": {
        "id": "hoiMhRRxoDIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "The objective of this project is to conduct sentiment analysis on airline customers' feedback on Twitter, employing various machine learning classifiers. We used the Twitter US Airline Sentiment dataset that had already been annotated with sentiment labels, but for the purpose of making the analysis more comprehensive we ignored them.\n",
        "\n",
        "---\n",
        "\n",
        "**Project Description**\n",
        "\n",
        "The project was divided into four main phases:\n",
        "  *   Preprocessing\n",
        "  *   Labeling\n",
        "  *   Feature Extraction\n",
        "  *   Classification\n",
        "\n",
        "---\n",
        "\n",
        "**Source of Data:**\n",
        "\n",
        "https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/data\n"
      ],
      "metadata": {
        "id": "7u5mZ657pB7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Python Libraries**"
      ],
      "metadata": {
        "id": "egnHG7iQxmU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Processing\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Data Labelling\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Analysis and Visualisation\n",
        "from tabulate import tabulate\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Feature Extraction\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Clasiffication\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "vEQCGVXXcObq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Uploading**"
      ],
      "metadata": {
        "id": "q6FjzWm1yDZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UI4htA5Q8Qkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e8bf47-da6d-4b46-e3c8-087dd8bdd84b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jxfjixcsepuo"
      },
      "outputs": [],
      "source": [
        "df_org = pd.read_csv('File Path/Tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_org[['tweet_id','airline','text']]"
      ],
      "metadata": {
        "id": "T_yVXijyzZtX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "un4g6uYSuwLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(tweet):\n",
        "    # Replace @username with empty string\n",
        "    tweet = re.sub('@[^\\s]+','', tweet)\n",
        "    # Replace www.* or https?://* with empty string\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', tweet)\n",
        "    # Replace #word with with empty string\n",
        "    tweet = re.sub('#([^\\s]+)','', tweet)\n",
        "    # Replace the numbers with empty string\n",
        "    tweet = re.sub('\\d+','', tweet)\n",
        "    # Replace punctuations Replace\n",
        "    tweet = re.sub('[^\\w\\s]','', tweet)\n",
        "    # Convert the alphabet to lowercase\n",
        "    tweet = str(tweet).lower()\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "9ITdzEz4x0z3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying processPost function for preprocessing\n",
        "df['clean_text'] = df['text'].apply(lambda x: pre_process(x))"
      ],
      "metadata": {
        "id": "YCF95NcayHVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace emoji with empty string\n",
        "df['clean_text'] = df['clean_text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
        "# Remove the daplicate raw\n",
        "df = df.drop_duplicates()\n",
        "# Tokenize the tweet text\n",
        "df['clean_text'] = df['clean_text'].apply(nltk.word_tokenize)\n",
        "# Remove the stop words\n",
        "stopwords_list = stopwords.words('english')\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x:[item for item in x if item not in stopwords_list])\n",
        "# lemmatize the tweet text\n",
        "lemmatizer  = WordNetLemmatizer()\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x:[lemmatizer .lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "0498nbQ-ihnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Labilling**"
      ],
      "metadata": {
        "id": "gu-3cpaXlnB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob Data Lablling\n",
        "df['Sentiment'] = ''\n",
        "for i,x in df.text.iteritems():\n",
        "    label = TextBlob(x)\n",
        "    df['Sentiment'][i] = label.sentiment.polarity\n",
        "def polarity_to_label(x):\n",
        "    if(x >= -1 and x < 0):\n",
        "        return 'Negative'\n",
        "    if(x == 0):\n",
        "        return 'Neutral'\n",
        "    if(x > 0 and x <= 1):\n",
        "        return 'Positive'\n",
        "df.Sentiment = df.Sentiment.apply(polarity_to_label)"
      ],
      "metadata": {
        "id": "0F23Um_JzgYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis and Visualisation**"
      ],
      "metadata": {
        "id": "twwM6zeWNM0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total number of tweets\n",
        "Tweets_Count = len(df)\n",
        "# Count number of unique users\n",
        "Unique_Users = len(df['tweet_id'].unique())\n",
        "dict_Tweets  = {'Name': ['Tweets_Count', 'Unique User_Count'], 'Count': [Tweets_Count, Unique_Users]}\n",
        "head = [\"Name\", \"Count\"]\n",
        "print(tabulate(dict_Tweets, headers=head, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "NAxTegmDNap3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud generation for all tweets\n",
        "all_words = df['clean_text'].to_string(index=False)\n",
        "word_cloud = WordCloud(width=800, height=500, random_state=21,max_font_size=110,colormap='tab20').generate(all_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpSdy-UrQYIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# People's feelings towards the airline companies\n",
        "matplotlib.style.use('tableau-colorblind10')\n",
        "df['Sentiment'].value_counts().plot.pie(autopct='%1.1f%%', wedgeprops={'linewidth':2.0, 'edgecolor': 'white'}, textprops={'fontsize':10})\n",
        "plt.title(\"People's feelings Towards the Airline Companies\", fontsize=14)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6XPR9Sf-Zl_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "pHtePXC17EF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a stacked bar plot\n",
        "matplotlib.style.use('tableau-colorblind10')\n",
        "crossplt = pd.crosstab(df['airline'], df['Sentiment'])\n",
        "crossplt.plot(kind=\"bar\", stacked=True)\n",
        "plt.yticks(fontsize=\"8\")\n",
        "plt.xticks(rotation=30, horizontalalignment=\"center\", fontsize=\"8\")\n",
        "plt.title(\"public Perception Per each Company\", fontsize=\"14\")\n",
        "plt.xlabel(\"Airline Companies\", fontsize=\"12\")\n",
        "plt.ylabel(\"Custmer Opinions\", fontsize=\"12\")\n",
        "plt.legend(fontsize=\"8\")"
      ],
      "metadata": {
        "id": "2EXvwz7XaCEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Features Extraction**"
      ],
      "metadata": {
        "id": "pVK8Lf1f6aj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF features extraction\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(df['clean_text'].astype('str'))\n",
        "unigramdataGet = tfidf.toarray()\n",
        "vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "features = pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
        "features[features>0] = 1\n",
        "pro = preprocessing.LabelEncoder()\n",
        "encpro = pro.fit_transform(df['Sentiment'])\n",
        "df['label'] = encpro\n",
        "y = df['Sentiment']\n",
        "X = features"
      ],
      "metadata": {
        "id": "lS5TZfVu-nPc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classification**"
      ],
      "metadata": {
        "id": "jQjMantpKqGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting Dataset into 70% Training and 30% Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=333)"
      ],
      "metadata": {
        "id": "yfeiHsFMLRzn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Algorithm\n",
        "nb = GaussianNB()\n",
        "nb = nb.fit(X_train , y_train)\n",
        "y_pred = nb.predict(X_test)\n",
        "nbg = nb.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(nb.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "8eswng0fQuEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LogisticRegression Algorithm\n",
        "LR = LogisticRegression(penalty = 'l2', C = 1)\n",
        "LR = LR.fit(X_train , y_train)\n",
        "y_pred = LR.predict(X_test)\n",
        "lr = LR.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(LR.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "KK4Z7GXeRmDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Algorithm\n",
        "SVCModel = SVC(kernel= 'poly',max_iter=100,C=1.0,gamma='auto') # it can be also linear,poly,sigmoid,precomputed\n",
        "SVCModel.fit(X_train, y_train)\n",
        "y_pred = SVCModel.predict(X_test)\n",
        "svm =SVCModel.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(SVCModel.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "XqHbecm3R0_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Algorithm\n",
        "MLPClassifierModel = MLPClassifier(activation='relu', # can be also identity , logistic , relu\n",
        "                                   solver='lbfgs',  # can be also sgd , adam\n",
        "                                   learning_rate='constant', # can be also invscaling , adaptive\n",
        "                                   early_stopping= False,max_iter=200,\n",
        "                                   alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=99)\n",
        "MLPClassifierModel.fit(X_train, y_train)\n",
        "y_pred = MLPClassifierModel.predict(X_test)\n",
        "nn =MLPClassifierModel.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(MLPClassifierModel.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "ODsfjhv7SAIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest Algorithm\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "RF =clf.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "pRuE0LgMSWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matplotlib.style.use('tableau-colorblind10')\n",
        "data = pd.DataFrame({'Classifiers': ['Naive Bayes', 'Logistic Regression', 'SVM', 'MLP', 'Random Forest'],\n",
        "        'Accuracy': [nbg, lr, svm, nn, RF]})\n",
        "ax = data.plot(x ='Classifiers', y='Accuracy', kind='bar', legend=False)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height() * 100:.1f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                 ha='center', va='center', fontsize=8, color='black', xytext=(0, 7),\n",
        "                 textcoords='offset points')\n",
        "plt.yticks(fontsize=\"8\")\n",
        "plt.xticks(rotation=30, horizontalalignment=\"center\", fontsize=\"8\")\n",
        "plt.title(\"Evaluating Classifiers Performance\", fontsize=\"14\")\n",
        "plt.xlabel(\"Classifiers\", fontsize=\"12\")\n",
        "plt.ylabel(\"Accuracy\", fontsize=\"12\")"
      ],
      "metadata": {
        "id": "qlfrx-xg27D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}